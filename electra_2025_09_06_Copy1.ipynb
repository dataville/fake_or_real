{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629926bc-93c1-452a-8e27-995f7f56548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa + Perplexity Ensemble\n",
      "=============================\n",
      "Loading training data...\n",
      "Loaded labels, shape: (95, 2)\n",
      "Loaded 95 training pairs\n",
      "Training ensemble of 5 models...\n",
      "\n",
      "=== Training Model 1/5 (seed=42) ===\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 190 text samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 251 chunks from texts\n",
      "Training for 10 epochs with 126 warmup steps...\n",
      "Epoch 1/10 - Loss: 2.4382, Accuracy: 0.6056, Best: 0.6056\n",
      "Epoch 2/10 - Loss: 2.0032, Accuracy: 0.6016, Best: 0.6056\n",
      "Epoch 3/10 - Loss: 1.2904, Accuracy: 0.6215, Best: 0.6215\n",
      "Epoch 4/10 - Loss: 0.8144, Accuracy: 0.7012, Best: 0.7012\n",
      "Epoch 5/10 - Loss: 0.6956, Accuracy: 0.7729, Best: 0.7729\n",
      "Epoch 6/10 - Loss: 0.6234, Accuracy: 0.8207, Best: 0.8207\n",
      "Epoch 7/10 - Loss: 0.5593, Accuracy: 0.8247, Best: 0.8247\n",
      "Epoch 8/10 - Loss: 0.5006, Accuracy: 0.8606, Best: 0.8606\n",
      "Epoch 9/10 - Loss: 0.4887, Accuracy: 0.8606, Best: 0.8606\n",
      "Epoch 10/10 - Loss: 0.4806, Accuracy: 0.8685, Best: 0.8685\n",
      "Training completed. Best accuracy: 0.8685\n",
      "Model 1 training completed!\n",
      "\n",
      "=== Training Model 2/5 (seed=98) ===\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 190 text samples\n",
      "Created 251 chunks from texts\n",
      "Training for 10 epochs with 126 warmup steps...\n",
      "Epoch 1/10 - Loss: 1.0699, Accuracy: 0.6016, Best: 0.6016\n",
      "Epoch 2/10 - Loss: 0.7335, Accuracy: 0.6733, Best: 0.6733\n",
      "Epoch 3/10 - Loss: 0.5433, Accuracy: 0.7371, Best: 0.7371\n",
      "Epoch 4/10 - Loss: 0.4448, Accuracy: 0.8287, Best: 0.8287\n",
      "Epoch 5/10 - Loss: 0.3925, Accuracy: 0.8725, Best: 0.8725\n",
      "Epoch 6/10 - Loss: 0.3128, Accuracy: 0.8964, Best: 0.8964\n",
      "Epoch 7/10 - Loss: 0.2951, Accuracy: 0.9124, Best: 0.9124\n",
      "Epoch 8/10 - Loss: 0.2868, Accuracy: 0.9084, Best: 0.9124\n",
      "Epoch 9/10 - Loss: 0.2226, Accuracy: 0.9283, Best: 0.9283\n",
      "Epoch 10/10 - Loss: 0.2021, Accuracy: 0.9323, Best: 0.9323\n",
      "Training completed. Best accuracy: 0.9323\n",
      "Model 2 training completed!\n",
      "\n",
      "=== Training Model 3/5 (seed=123) ===\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 190 text samples\n",
      "Created 251 chunks from texts\n",
      "Training for 10 epochs with 126 warmup steps...\n",
      "Epoch 1/10 - Loss: 0.6681, Accuracy: 0.6056, Best: 0.6056\n",
      "Epoch 2/10 - Loss: 0.5452, Accuracy: 0.6733, Best: 0.6733\n",
      "Epoch 3/10 - Loss: 0.4509, Accuracy: 0.7490, Best: 0.7490\n",
      "Epoch 4/10 - Loss: 0.4254, Accuracy: 0.7689, Best: 0.7689\n",
      "Epoch 5/10 - Loss: 0.3586, Accuracy: 0.8486, Best: 0.8486\n",
      "Epoch 6/10 - Loss: 0.2916, Accuracy: 0.8924, Best: 0.8924\n",
      "Epoch 7/10 - Loss: 0.2524, Accuracy: 0.8924, Best: 0.8924\n",
      "Epoch 8/10 - Loss: 0.2424, Accuracy: 0.9044, Best: 0.9044\n",
      "Epoch 9/10 - Loss: 0.2303, Accuracy: 0.9163, Best: 0.9163\n",
      "Epoch 10/10 - Loss: 0.2233, Accuracy: 0.9124, Best: 0.9163\n",
      "Training completed. Best accuracy: 0.9163\n",
      "Model 3 training completed!\n",
      "\n",
      "=== Training Model 4/5 (seed=225) ===\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 190 text samples\n",
      "Created 251 chunks from texts\n",
      "Training for 10 epochs with 126 warmup steps...\n",
      "Epoch 1/10 - Loss: 0.7910, Accuracy: 0.3267, Best: 0.3267\n",
      "Epoch 2/10 - Loss: 0.5756, Accuracy: 0.6016, Best: 0.6016\n",
      "Epoch 3/10 - Loss: 0.4716, Accuracy: 0.7131, Best: 0.7131\n",
      "Epoch 4/10 - Loss: 0.3917, Accuracy: 0.8486, Best: 0.8486\n",
      "Epoch 5/10 - Loss: 0.3262, Accuracy: 0.8606, Best: 0.8606\n",
      "Epoch 6/10 - Loss: 0.2776, Accuracy: 0.8845, Best: 0.8845\n",
      "Epoch 7/10 - Loss: 0.2325, Accuracy: 0.9084, Best: 0.9084\n",
      "Epoch 8/10 - Loss: 0.2497, Accuracy: 0.9163, Best: 0.9163\n",
      "Epoch 9/10 - Loss: 0.1985, Accuracy: 0.9323, Best: 0.9323\n",
      "Epoch 10/10 - Loss: 0.2225, Accuracy: 0.9243, Best: 0.9323\n",
      "Training completed. Best accuracy: 0.9323\n",
      "Model 4 training completed!\n",
      "\n",
      "=== Training Model 5/5 (seed=456) ===\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Created 190 text samples\n",
      "Created 251 chunks from texts\n",
      "Training for 10 epochs with 126 warmup steps...\n",
      "Epoch 1/10 - Loss: 0.5558, Accuracy: 0.6175, Best: 0.6175\n",
      "Epoch 2/10 - Loss: 0.5051, Accuracy: 0.6972, Best: 0.6972\n",
      "Epoch 3/10 - Loss: 0.4666, Accuracy: 0.7251, Best: 0.7251\n",
      "Epoch 4/10 - Loss: 0.3899, Accuracy: 0.8367, Best: 0.8367\n",
      "Epoch 5/10 - Loss: 0.3120, Accuracy: 0.8805, Best: 0.8805\n",
      "Epoch 6/10 - Loss: 0.2784, Accuracy: 0.8924, Best: 0.8924\n",
      "Epoch 7/10 - Loss: 0.2424, Accuracy: 0.9044, Best: 0.9044\n",
      "Epoch 8/10 - Loss: 0.2181, Accuracy: 0.9203, Best: 0.9203\n",
      "Epoch 9/10 - Loss: 0.2202, Accuracy: 0.9243, Best: 0.9243\n",
      "Epoch 10/10 - Loss: 0.2267, Accuracy: 0.9283, Best: 0.9283\n",
      "Training completed. Best accuracy: 0.9283\n",
      "Model 5 training completed!\n",
      "\n",
      "Ensemble training completed! Trained 5 models.\n",
      "Making ensemble predictions on test data...\n",
      "Ensemble submission saved to: ensemble_roberta_perplexity_20250906_135109.csv\n",
      "Predicted 1068 test samples\n",
      "Ensemble training and prediction completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    GPT2LMHeadModel, GPT2TokenizerFast,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def read_texts_from_dir(base_dir, labels_csv):\n",
    "    \"\"\"Load texts and labels from directory structure - EXACTLY like your original\"\"\"\n",
    "    labels_df = pd.read_csv(labels_csv)\n",
    "    print(f\"Loaded labels, shape: {labels_df.shape}\")\n",
    "    records = []\n",
    "    \n",
    "    for folder in sorted(os.listdir(base_dir)):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        try:\n",
    "            article_id = int(folder.split(\"_\")[-1])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "        file1_path = os.path.join(folder_path, \"file_1.txt\")\n",
    "        file2_path = os.path.join(folder_path, \"file_2.txt\")\n",
    "        if not (os.path.exists(file1_path) and os.path.exists(file2_path)):\n",
    "            continue\n",
    "            \n",
    "        with open(file1_path, \"r\", encoding=\"utf-8\") as f1, open(file2_path, \"r\", encoding=\"utf-8\") as f2:\n",
    "            text1 = f1.read()\n",
    "            text2 = f2.read()\n",
    "            \n",
    "        real_text_id = labels_df.loc[labels_df['id'] == article_id, 'real_text_id'].values\n",
    "        if len(real_text_id) == 0:\n",
    "            continue\n",
    "            \n",
    "        label = 0 if real_text_id[0] == 1 else 1\n",
    "        records.append({\n",
    "            \"id\": article_id,\n",
    "            \"file_1\": text1,\n",
    "            \"file_2\": text2,\n",
    "            \"label\": label\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(records).sort_values(\"id\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    \"\"\"Calculate perplexity features that showed high correlation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.gpt2_model.eval()\n",
    "        \n",
    "    def get_perplexity(self, text):\n",
    "        \"\"\"Get perplexity score for text\"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return 1000.0  # Very high perplexity for empty/short texts\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Use first 400 words\n",
    "                words = text.split()[:400]\n",
    "                truncated_text = ' '.join(words)\n",
    "                \n",
    "                inputs = self.gpt2_tokenizer(truncated_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "                if inputs['input_ids'].size(1) <= 1:\n",
    "                    return 1000.0\n",
    "                \n",
    "                outputs = self.gpt2_model(**inputs, labels=inputs['input_ids'])\n",
    "                perplexity = torch.exp(outputs.loss).item()\n",
    "                \n",
    "                # Clamp to reasonable range\n",
    "                return max(1.0, min(perplexity, 1000.0))\n",
    "                \n",
    "        except Exception:\n",
    "            return 500.0  # High perplexity if calculation fails\n",
    "\n",
    "class TextChunkDataset(Dataset):\n",
    "    \"\"\"EXACTLY your original chunking dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, perplexity_calc, max_length=512, chunk_overlap=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.perplexity_calc = perplexity_calc\n",
    "        \n",
    "        self.data = []\n",
    "        for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "            # Calculate perplexity ONCE per text\n",
    "            text_perplexity = self.perplexity_calc.get_perplexity(text)\n",
    "            \n",
    "            chunks = self.chunk_text(text)\n",
    "            for chunk in chunks:\n",
    "                self.data.append({\n",
    "                    'text': chunk,\n",
    "                    'label': label,\n",
    "                    'text_id': i,\n",
    "                    'perplexity': text_perplexity  # Add perplexity as feature\n",
    "                })\n",
    "    \n",
    "    def chunk_text(self, text, max_words=400):\n",
    "        \"\"\"Split text into overlapping chunks - EXACTLY like your original\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) <= max_words:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = min(start + max_words, len(words))\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            if end >= len(words):\n",
    "                break\n",
    "            start = end - self.chunk_overlap\n",
    "        return chunks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(item['label'], dtype=torch.long),\n",
    "            'text_id': item['text_id'],\n",
    "            'perplexity': torch.tensor(item['perplexity'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class EnhancedRobertaModel(nn.Module):\n",
    "    \"\"\"Enhanced model + perplexity feature - works with RoBERTa or DeBERTa\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='roberta-large'):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        \n",
    "        # Get the correct hidden size from the model config\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        \n",
    "        # Add a small network to combine model output with perplexity\n",
    "        self.perplexity_layer = nn.Linear(1, 64)\n",
    "        self.combine_layer = nn.Linear(hidden_size + 64, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, perplexity, labels=None):\n",
    "        # Get base model features (not final classification)\n",
    "        # Handle both RoBERTa and DeBERTa architectures\n",
    "        if hasattr(self.base_model, 'roberta'):\n",
    "            # RoBERTa\n",
    "            base_outputs = self.base_model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        elif hasattr(self.base_model, 'deberta'):\n",
    "            # DeBERTa\n",
    "            base_outputs = self.base_model.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            # Fallback - get hidden states from the base model\n",
    "            base_outputs = self.base_model.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Average pooling of last hidden state\n",
    "        base_features = base_outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # Process perplexity\n",
    "        perplexity_features = torch.relu(self.perplexity_layer(perplexity.unsqueeze(-1)))\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([base_features, perplexity_features], dim=1)\n",
    "        logits = self.combine_layer(combined)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "class ImprovedRobertaDetector:\n",
    "    \"\"\"Your original approach + perplexity features\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='roberta-large', max_length=512):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = EnhancedRobertaModel(model_name).to(self.device)\n",
    "        self.max_length = max_length\n",
    "        self.perplexity_calc = PerplexityCalculator()\n",
    "        \n",
    "    def prepare_data(self, data_df):\n",
    "        \"\"\"Prepare training data from DataFrame - EXACTLY like your original\"\"\"\n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in data_df.iterrows():\n",
    "            # File 1\n",
    "            texts.append(row['file_1'])\n",
    "            labels.append(1 if row['label'] == 0 else 0)  # 1 if file_1 is real\n",
    "            \n",
    "            # File 2  \n",
    "            texts.append(row['file_2'])\n",
    "            labels.append(1 if row['label'] == 1 else 0)  # 1 if file_2 is real\n",
    "            \n",
    "        return texts, labels\n",
    "    \n",
    "    def train(self, data_df, epochs=10, batch_size=3, learning_rate=2e-6, warmup_ratio=0.15):\n",
    "        \"\"\"Train with your original hyperparameters\"\"\"\n",
    "        print(f\"Preparing training data...\")\n",
    "        texts, labels = self.prepare_data(data_df)\n",
    "        print(f\"Created {len(texts)} text samples\")\n",
    "        \n",
    "        # Create dataset with chunking + perplexity\n",
    "        dataset = TextChunkDataset(texts, labels, self.tokenizer, self.perplexity_calc, self.max_length)\n",
    "        print(f\"Created {len(dataset)} chunks from texts\")\n",
    "        \n",
    "        # Create DataLoader\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Setup training - EXACTLY like your original\n",
    "        self.model.train()\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "        total_steps = len(dataloader) * epochs\n",
    "        warmup_steps = int(total_steps * warmup_ratio)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        print(f\"Training for {epochs} epochs with {warmup_steps} warmup steps...\")\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                perplexity = batch['perplexity'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    perplexity=perplexity,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs['loss']\n",
    "                logits = outputs['logits']\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct_predictions += (predictions == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Best: {best_accuracy:.4f}\")\n",
    "        \n",
    "        print(f\"Training completed. Best accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    def predict_text(self, text):\n",
    "        \"\"\"Predict probability that a single text is real\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get perplexity for the full text\n",
    "        text_perplexity = self.perplexity_calc.get_perplexity(text)\n",
    "        \n",
    "        # Chunk the text\n",
    "        chunks = self.chunk_text(text)\n",
    "        chunk_probs = []\n",
    "        chunk_confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for chunk in chunks:\n",
    "                encoding = self.tokenizer(\n",
    "                    chunk,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "                \n",
    "                perplexity_tensor = torch.tensor([text_perplexity], dtype=torch.float).to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=encoding['input_ids'],\n",
    "                    attention_mask=encoding['attention_mask'],\n",
    "                    perplexity=perplexity_tensor\n",
    "                )\n",
    "                \n",
    "                probs = torch.softmax(outputs['logits'], dim=1)\n",
    "                prob_real = probs[0][1].item()  # Probability of being real\n",
    "                confidence = max(probs[0]).item()  # Confidence\n",
    "                \n",
    "                chunk_probs.append(prob_real)\n",
    "                chunk_confidences.append(confidence)\n",
    "        \n",
    "        if not chunk_probs:\n",
    "            return 0.5\n",
    "        \n",
    "        # Weighted average by confidence - EXACTLY like your original\n",
    "        total_weight = sum(chunk_confidences)\n",
    "        if total_weight > 0:\n",
    "            weighted_prob = sum(p * c for p, c in zip(chunk_probs, chunk_confidences)) / total_weight\n",
    "        else:\n",
    "            weighted_prob = np.mean(chunk_probs)\n",
    "        \n",
    "        return weighted_prob\n",
    "    \n",
    "    def chunk_text(self, text, max_words=400, overlap=50):\n",
    "        \"\"\"Split text into overlapping chunks - EXACTLY like your original\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) <= max_words:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = min(start + max_words, len(words))\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            if end >= len(words):\n",
    "                break\n",
    "            start = end - overlap\n",
    "        return chunks\n",
    "    \n",
    "    def predict_pairs(self, text1, text2):\n",
    "        \"\"\"Predict which text in a pair is more likely to be real\"\"\"\n",
    "        prob1 = self.predict_text(text1)\n",
    "        prob2 = self.predict_text(text2)\n",
    "        \n",
    "        return [prob1, prob2]\n",
    "    \n",
    "    def predict_test_data(self, test_dir, output_file=None):\n",
    "        \"\"\"Make predictions on test data - EXACTLY like your original\"\"\"\n",
    "        print(\"Making predictions on test data...\")\n",
    "        results = []\n",
    "        \n",
    "        for folder in sorted(os.listdir(test_dir)):\n",
    "            folder_path = os.path.join(test_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                folder_id = int(folder.split(\"_\")[-1])\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            file1_path = os.path.join(folder_path, \"file_1.txt\")\n",
    "            file2_path = os.path.join(folder_path, \"file_2.txt\")\n",
    "            \n",
    "            if not (os.path.exists(file1_path) and os.path.exists(file2_path)):\n",
    "                continue\n",
    "            \n",
    "            # Read texts\n",
    "            with open(file1_path, \"r\", encoding=\"utf-8\") as f1:\n",
    "                text1 = f1.read()\n",
    "            with open(file2_path, \"r\", encoding=\"utf-8\") as f2:\n",
    "                text2 = f2.read()\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = self.predict_pairs(text1, text2)\n",
    "            \n",
    "            # Determine real text\n",
    "            real_text_id = 1 if probs[0] > probs[1] else 2\n",
    "            \n",
    "            results.append({\n",
    "                'id': folder_id,\n",
    "                'real_text_id': int(real_text_id)\n",
    "            })\n",
    "        \n",
    "        # Create submission\n",
    "        submission = pd.DataFrame(results).sort_values('id')\n",
    "        \n",
    "        if output_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = f\"roberta_plus_perplexity_{timestamp}.csv\"\n",
    "        \n",
    "        submission.to_csv(output_file, index=False)\n",
    "        print(f\"Submission saved to: {output_file}\")\n",
    "        print(f\"Predicted {len(submission)} test samples\")\n",
    "        \n",
    "        return submission\n",
    "\n",
    "class EnsembleDetector:\n",
    "    \"\"\"Ensemble of multiple RoBERTa + Perplexity models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='roberta-large', n_models=3):\n",
    "        self.model_name = model_name\n",
    "        self.n_models = n_models\n",
    "        self.models = []\n",
    "        self.seeds = [42, 98, 123, 225, 456]  # Fixed seeds for reproducibility\n",
    "        \n",
    "    def train_ensemble(self, data_df, epochs=10, batch_size=3, learning_rate=2e-6, warmup_ratio=0.15):\n",
    "        \"\"\"Train multiple models with different random seeds\"\"\"\n",
    "        print(f\"Training ensemble of {self.n_models} models...\")\n",
    "        \n",
    "        for i, seed in enumerate(self.seeds[:self.n_models]):\n",
    "            print(f\"\\n=== Training Model {i+1}/{self.n_models} (seed={seed}) ===\")\n",
    "            \n",
    "            # Set random seeds\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            import random\n",
    "            random.seed(seed)\n",
    "            \n",
    "            # Create and train model\n",
    "            detector = ImprovedRobertaDetector(model_name=self.model_name)\n",
    "            detector.train(data_df, epochs=epochs, batch_size=batch_size, \n",
    "                          learning_rate=learning_rate, warmup_ratio=warmup_ratio)\n",
    "            \n",
    "            self.models.append(detector)\n",
    "            print(f\"Model {i+1} training completed!\")\n",
    "        \n",
    "        print(f\"\\nEnsemble training completed! Trained {len(self.models)} models.\")\n",
    "    \n",
    "    def predict_pairs(self, text1, text2):\n",
    "        \"\"\"Predict using ensemble - average predictions from all models\"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models trained! Call train_ensemble first.\")\n",
    "        \n",
    "        all_probs = []\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            probs = model.predict_pairs(text1, text2)\n",
    "            all_probs.append(probs)\n",
    "        \n",
    "        # Average predictions across all models\n",
    "        avg_probs = np.mean(all_probs, axis=0)\n",
    "        return avg_probs.tolist()\n",
    "    \n",
    "    def predict_test_data(self, test_dir, output_file=None):\n",
    "        \"\"\"Make ensemble predictions on test data\"\"\"\n",
    "        print(\"Making ensemble predictions on test data...\")\n",
    "        results = []\n",
    "        \n",
    "        for folder in sorted(os.listdir(test_dir)):\n",
    "            folder_path = os.path.join(test_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                folder_id = int(folder.split(\"_\")[-1])\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            file1_path = os.path.join(folder_path, \"file_1.txt\")\n",
    "            file2_path = os.path.join(folder_path, \"file_2.txt\")\n",
    "            \n",
    "            if not (os.path.exists(file1_path) and os.path.exists(file2_path)):\n",
    "                continue\n",
    "            \n",
    "            # Read texts\n",
    "            with open(file1_path, \"r\", encoding=\"utf-8\") as f1:\n",
    "                text1 = f1.read()\n",
    "            with open(file2_path, \"r\", encoding=\"utf-8\") as f2:\n",
    "                text2 = f2.read()\n",
    "            \n",
    "            # Get ensemble probabilities\n",
    "            probs = self.predict_pairs(text1, text2)\n",
    "            \n",
    "            # Determine real text\n",
    "            real_text_id = 1 if probs[0] > probs[1] else 2\n",
    "            \n",
    "            results.append({\n",
    "                'id': folder_id,\n",
    "                'real_text_id': int(real_text_id),\n",
    "                'confidence': max(probs)\n",
    "            })\n",
    "        \n",
    "        # Create submission\n",
    "        submission = pd.DataFrame(results)[['id', 'real_text_id']].sort_values('id')\n",
    "        \n",
    "        if output_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = f\"ensemble_roberta_perplexity_{timestamp}.csv\"\n",
    "        \n",
    "        submission.to_csv(output_file, index=False)\n",
    "        print(f\"Ensemble submission saved to: {output_file}\")\n",
    "        print(f\"Predicted {len(submission)} test samples\")\n",
    "        \n",
    "        return submission\n",
    "\n",
    "# Main execution\n",
    "print(\"RoBERTa + Perplexity Ensemble\")\n",
    "print(\"=============================\")\n",
    "\n",
    "# Choose between single model or ensemble\n",
    "USE_ENSEMBLE = True  # Set to False for single model\n",
    "\n",
    "train_dir = \"/mnt/Monolith/ML/kaggle/input/fake-or-real-the-impostor-hunt/data/train\"\n",
    "labels_csv = \"/mnt/Monolith/ML/kaggle/input/fake-or-real-the-impostor-hunt/data/train.csv\"\n",
    "test_dir = \"/mnt/Monolith/ML/kaggle/input/fake-or-real-the-impostor-hunt/data/test\"\n",
    "\n",
    "if os.path.exists(train_dir) and os.path.exists(labels_csv):\n",
    "    print(\"Loading training data...\")\n",
    "    data_df = read_texts_from_dir(train_dir, labels_csv)\n",
    "    print(f\"Loaded {len(data_df)} training pairs\")\n",
    "    \n",
    "    if len(data_df) > 0:\n",
    "        if USE_ENSEMBLE:\n",
    "            # Train ensemble\n",
    "            ensemble = EnsembleDetector(model_name='google/electra-large-discriminator', n_models=5)\n",
    "            ensemble.train_ensemble(data_df, epochs=10, batch_size=3, learning_rate=2e-6, warmup_ratio=0.15)\n",
    "            \n",
    "            if os.path.exists(test_dir):\n",
    "                submission = ensemble.predict_test_data(test_dir)\n",
    "                print(\"Ensemble training and prediction completed!\")\n",
    "        else:\n",
    "            # Train single model (your original approach)\n",
    "            detector = ImprovedRobertaDetector()\n",
    "            detector.train(data_df, epochs=10, batch_size=3, learning_rate=2e-6, warmup_ratio=0.15)\n",
    "            \n",
    "            if os.path.exists(test_dir):\n",
    "                submission = detector.predict_test_data(test_dir)\n",
    "                print(\"Single model training and prediction completed!\")\n",
    "    else:\n",
    "        print(\"No training data found\")\n",
    "else:\n",
    "    print(\"Training data not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
